{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPoj2CUYOgddnDK02BgMpVL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zen-Sherbert/Proteus-Attention/blob/main/TinyPlayground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0uvnxIYXMql"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_K-gpLELABM",
        "outputId": "73a86396-350d-486c-b5f5-4af200c0d87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Proteus-Attention'...\n",
            "remote: Enumerating objects: 127, done.\u001b[K\n",
            "remote: Counting objects: 100% (127/127), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 127 (delta 39), reused 92 (delta 19), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (127/127), 150.01 KiB | 6.82 MiB/s, done.\n",
            "Resolving deltas: 100% (39/39), done.\n",
            "/content/Proteus-Attention\n",
            "Repository files:\n",
            "examples/  MANIFEST.in\t   scripts/  tests/\n",
            "LICENSE    pyproject.toml  src/      tinytoy_run.txt\n"
          ]
        }
      ],
      "source": [
        "# 1. Clone your GitHub repository\n",
        "!git clone https://github.com/Zen-Sherbert/Proteus-Attention.git\n",
        "\n",
        "# 2. Change the working directory to the cloned folder\n",
        "%cd Proteus-Attention\n",
        "\n",
        "# 3. (Optional) List the files to show the user everything is there\n",
        "print(\"Repository files:\")\n",
        "!ls -F"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5da73279",
        "outputId": "bd468082-c8ff-42f5-ab5a-0cd4606fc733"
      },
      "source": [
        "# Install the proteus_attention package\n",
        "!pip install ."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/Proteus-Attention\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.12/dist-packages (from proteus-attention==0.1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1->proteus-attention==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1->proteus-attention==0.1.0) (3.0.3)\n",
            "Building wheels for collected packages: proteus-attention\n",
            "  Building wheel for proteus-attention (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for proteus-attention: filename=proteus_attention-0.1.0-py3-none-any.whl size=74525 sha256=6d29034ed42007fb9a820cb698ab41b4d1f26ef1803b454f86ae35c423feee4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/38/b5/a9e731a0381ba75a59307d7a0512b8a0ac1e70c603ff48dc4f\n",
            "Successfully built proteus-attention\n",
            "Installing collected packages: proteus-attention\n",
            "Successfully installed proteus-attention-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cd9c01c",
        "outputId": "878b1c29-bc48-487b-8329-20d50fc170bd"
      },
      "source": [
        "# 4. Run the tiny toy script on the GPU\n",
        "!python src/proteus_attention/kernels/tinytoy.py"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Config] SDPA fast-path disabled; benchmarking Flux kernels only.\n",
            "--- Starting Attention Benchmark ---\n",
            "Device: cuda\n",
            "Sequence lengths: [128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576]\n",
            "\n",
            "[Probe] Standard testing seq_len=128 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=256 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=512 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=1024 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=2048 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=4096 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=8192 batch=1 runs=1\n",
            "[Probe] Standard testing seq_len=16384 batch=1 runs=1\n",
            "[Probe] Standard testing seq_len=32768 batch=1 runs=1\n",
            "[Probe] Standard failed at sequence length 32768: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.44 GiB is free. Process 42344 has 308.00 MiB memory in use. Of the allocated memory 138.13 MiB is allocated by PyTorch, and 43.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "[Probe] Standard safe up to sequence length 16384 on this device.\n",
            "[Probe] DMoAH testing seq_len=128 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=256 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=512 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=1024 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=2048 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=4096 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=8192 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=16384 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=32768 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=65536 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=131072 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=262144 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=524288 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=1048576 batch=1 runs=1\n",
            "[Probe] DMoAH failed at sequence length 1048576: CUDA out of memory. Tried to allocate 6.60 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.96 GiB is free. Process 42344 has 9.78 GiB memory in use. Of the allocated memory 9.59 GiB is allocated by PyTorch, and 37.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "[Probe] DMoAH safe up to sequence length 524288 on this device.\n",
            "Seq Len    | Model                    | Latency (ms)    | Seq/s        | Tok/s           | Memory (MB)     | Active K   | Tokens     | Mode       | Backend                             \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[Warmup] Priming flux kernel at seq_len=8192...\n",
            "128        | Standard Attention       | 0.44            | 9061.16      | 1159828.54      | 14.13           | -          | -          | -          | torch.nn                            \n",
            "128        | DMoAH (BF16)             | 5.26            | 759.83       | 97257.99        | 18.33           | 8          | 1.00       | dense      | triton (max_rows=128, density=0.34, unique=32, tokens=1.00, dna_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=128 backend=triton\n",
            "128        | DMoAH (INT8)             | 5.01            | 798.82       | 102249.50       | 18.33           | 8          | 1.00       | dense      | triton (max_rows=128, density=0.33, unique=32, tokens=1.00, dna_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=128 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "256        | Standard Attention       | 0.89            | 4482.23      | 1147449.60      | 18.63           | -          | -          | -          | torch.nn                            \n",
            "256        | DMoAH (BF16)             | 5.12            | 781.84       | 200149.85       | 26.27           | 8          | 1.00       | dense      | triton (max_rows=256, density=0.31, unique=32, tokens=1.00, dna_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=256 backend=triton\n",
            "256        | DMoAH (INT8)             | 5.96            | 670.82       | 171730.44       | 26.27           | 8          | 1.00       | dense      | triton (max_rows=256, density=0.32, unique=32, tokens=1.00, dna_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=256 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "512        | Standard Attention       | 2.49            | 1607.92      | 823256.92       | 35.63           | -          | -          | -          | torch.nn                            \n",
            "512        | DMoAH (BF16)             | 7.97            | 502.12       | 257087.08       | 50.61           | 7          | 0.80       | sparse     | triton (max_rows=117, density=0.23, unique=32, tokens=0.80, dna_blend=0.00, alpha=0.03)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=512 backend=triton\n",
            "512        | DMoAH (INT8)             | 6.57            | 608.91       | 311763.68       | 56.91           | 7          | 0.80       | sparse     | triton (max_rows=107, density=0.21, unique=32, int8, tokens=0.80, dna_blend=0.00, alpha=0.03)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=512 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "1024       | Standard Attention       | 7.17            | 557.51       | 570891.86       | 93.81           | -          | -          | -          | torch.nn                            \n",
            "1024       | DMoAH (BF16)             | 8.82            | 453.38       | 464261.71       | 91.94           | 7          | 0.80       | sparse     | triton (max_rows=210, density=0.21, unique=32, tokens=0.80, dna_blend=0.00, alpha=0.10)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=1024 backend=triton\n",
            "1024       | DMoAH (INT8)             | 10.47           | 382.15       | 391318.65       | 104.99          | 7          | 0.80       | sparse     | triton (max_rows=205, density=0.20, unique=32, int8, tokens=0.80, dna_blend=0.00, alpha=0.10)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=1024 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "2048       | Standard Attention       | 23.46           | 170.50       | 349192.11       | 305.98          | -          | -          | -          | torch.nn                            \n",
            "2048       | DMoAH (BF16)             | 15.65           | 255.56       | 523384.42       | 177.46          | 6          | 0.60       | sparse     | triton (max_rows=269, density=0.13, unique=32, tokens=0.60, dna_blend=0.00, alpha=0.23)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=2048 backend=triton\n",
            "2048       | DMoAH (INT8)             | 19.08           | 209.62       | 429304.69       | 203.56          | 6          | 0.60       | sparse     | triton (max_rows=274, density=0.13, unique=32, int8, tokens=0.60, dna_blend=0.00, alpha=0.23)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=2048 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "4096       | Standard Attention       | 86.00           | 46.51        | 190508.54       | 1114.08         | -          | -          | -          | torch.nn                            \n",
            "4096       | DMoAH (BF16)             | 39.47           | 101.35       | 415123.96       | 360.83          | 6          | 0.60       | sparse     | triton (max_rows=516, density=0.13, unique=32, tokens=0.60, dna_blend=0.00, alpha=0.48)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=4096 backend=triton\n",
            "4096       | DMoAH (INT8)             | 45.06           | 88.76        | 363565.76       | 413.28          | 6          | 0.60       | sparse     | triton (max_rows=516, density=0.13, unique=32, int8, tokens=0.60, dna_blend=0.00, alpha=0.48)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=4096 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "8192       | Standard Attention       | 82.02           | 12.19        | 99878.80        | 1098.54         | -          | -          | -          | torch.nn                            \n",
            "8192       | DMoAH (BF16)             | 24.95           | 40.08        | 328308.67       | 298.36          | 6          | 0.45       | sparse     | triton (max_rows=750, density=0.09, unique=32, tokens=0.45, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=8192 backend=triton\n",
            "8192       | DMoAH (INT8)             | 27.52           | 36.34        | 297659.67       | 324.14          | 6          | 0.45       | sparse     | triton (max_rows=740, density=0.09, unique=32, int8, tokens=0.45, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=8192 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "16384      | Standard Attention       | 305.87          | 3.27         | 53564.96        | 4209.97         | -          | -          | -          | torch.nn                            \n",
            "16384      | DMoAH (BF16)             | 68.53           | 14.59        | 239079.83       | 816.44          | 5          | 0.45       | sparse     | triton (max_rows=1274, density=0.08, unique=32, tokens=0.45, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=16384 backend=triton\n",
            "16384      | DMoAH (INT8)             | 73.15           | 13.67        | 223986.45       | 868.67          | 5          | 0.45       | sparse     | triton (max_rows=1266, density=0.08, unique=32, int8, tokens=0.45, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=16384 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[Notice] Standard attention hit its limit at seq_len=32768 (last safe ~16384).\n",
            "32768      | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "32768      | DMoAH (BF16)             | 108.88          | 9.18         | 300962.31       | 936.76          | 2          | 0.21       | flux       | triton (max_rows=479, density=0.01, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=32768 backend=triton\n",
            "32768      | DMoAH (INT8)             | 109.30          | 9.15         | 299796.44       | 936.12          | 2          | 0.21       | flux       | triton (max_rows=475, density=0.01, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=32768 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "65536      | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "65536      | DMoAH (BF16)             | 215.59          | 4.64         | 303977.37       | 1493.42         | 2          | 0.21       | flux       | triton (max_rows=1127, density=0.02, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=65536 backend=triton\n",
            "65536      | DMoAH (INT8)             | 216.33          | 4.62         | 302949.43       | 1493.56         | 2          | 0.21       | flux       | triton (max_rows=1175, density=0.02, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=65536 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "131072     | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "131072     | DMoAH (BF16)             | 438.19          | 2.28         | 299120.64       | 2789.14         | 2          | 0.21       | flux       | triton (max_rows=4218, density=0.03, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=131072 backend=triton\n",
            "131072     | DMoAH (INT8)             | 439.93          | 2.27         | 297937.09       | 2789.69         | 2          | 0.21       | flux       | triton (max_rows=3190, density=0.02, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=131072 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "262144     | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "262144     | DMoAH (BF16)             | 901.30          | 1.11         | 290850.62       | 5543.89         | 2          | 0.21       | flux       | triton (max_rows=42155, density=0.16, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=262144 backend=triton\n",
            "262144     | DMoAH (INT8)             | 904.13          | 1.11         | 289941.06       | 5544.52         | 2          | 0.21       | flux       | triton (max_rows=39667, density=0.15, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=262144 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "524288     | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "524288     | DMoAH (BF16)             | 1855.88         | 0.54         | 282500.39       | 11054.36        | 2          | 0.21       | flux       | triton (max_rows=17841, density=0.03, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=524288 backend=triton\n",
            "524288     | DMoAH (INT8)             | 1851.37         | 0.54         | 283188.73       | 11057.75        | 2          | 0.21       | flux       | triton (max_rows=90749, density=0.17, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=524288 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "1048576    | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "1048576    | DMoAH (BF16)             | OOM             | -            | -               | -               | -          | -          | auto       | dmoah-skip                          \n",
            "[Notice] DMoAH (BF16) skipping seq_len=1048576 due to limit.\n",
            "1048576    | DMoAH (BF16)             | OOM             | -            | -               | -               | -          | -          | -          | dmoah-limit                         \n",
            "1048576    | DMoAH (INT8)             | OOM             | -            | -               | -               | -          | -          | auto       | dmoah-skip                          \n",
            "[Notice] DMoAH (INT8) skipping seq_len=1048576 due to limit.\n",
            "1048576    | DMoAH (INT8)             | OOM             | -            | -               | -               | -          | -          | -          | dmoah-limit                         \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Summary] Standard safe length: 16384; DMoAH safe length: 524288; Chunked Flux safe length: disabled.\n",
            "[Plot] Saved latency plot to reports/tinytoy/tinytoy_summary_20251104-010102.png\n",
            "\n",
            "[Summary] Detailed JSON report saved to reports/tinytoy/tinytoy_summary_20251104-010102.json\n",
            "[Summary] Captured 26 AutoLog entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5149d2c2",
        "outputId": "be19a0f2-a6c2-424f-c582-9e7befd0b76e"
      },
      "source": [
        "# Run the chunked_flux_smoke.py script\n",
        "!python scripts/chunked_flux_smoke.py"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunked Flux: 100% 16/16 [00:02<00:00,  7.45chunk/s]\n",
            "\n",
            "=== Chunked Flux Smoke Test ===\n",
            "device              : cuda\n",
            "sequence length     : 1000000\n",
            "retained tokens     : 32768 (3.28%)\n",
            "chunks processed    : 16\n",
            "stream latency (ms) : 2149.73\n",
            "stream throughput   : 465,175.64 tok/s\n",
            "final latency (ms)  : 110.69\n",
            "final throughput    : 296,036.17 tok/s\n",
            "final peak memory   : 1257.5 MB\n",
            "overall throughput  : 427,471.08 tok/s\n",
            "==============================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52166761",
        "outputId": "d85639b8-591c-41bc-f94f-72f58fd8bdbc"
      },
      "source": [
        "# Run the chunked_flux_tests.py script\n",
        "!python scripts/chunked_flux_tests.py"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config: seq_len=8,192 chunk_len=1,024 buffer_tokens=8 per_chunk_budget=2 (keep ratio 1.000)\n",
            "=== Needle recall (high router score) ===\n",
            "retained sentinel? True | keep_indices=[1047, 2891, 3937, 4357, 4369, 5231, 7174, 7233]\n",
            "\n",
            "=== DNA teleportation hypothesis ===\n",
            "without DNA retained? False | with DNA retained? True\n",
            "\n",
            "=== Ordering sanity (RoPE alignment) ===\n",
            "keep_indices monotonic? True\n",
            "\n",
            "=== Needle cluster stress test ===\n",
            "[cluster] needles kept 1/10 (recall=0.10) with per_chunk_budget=1\n",
            "[cluster-adapt] needles kept 8/10 (recall=0.80) margin=0.1 max_extra=8\n",
            "\n",
            "=== Fading signal sweep ===\n",
            "[fading] sensitivity sweep:\n",
            "  boost=5.00 -> recall 100.00% (20/20)\n",
            "  boost=2.00 -> recall 100.00% (20/20)\n",
            "  boost=1.00 -> recall 100.00% (20/20)\n",
            "  boost=0.50 -> recall 100.00% (20/20)\n",
            "  boost=0.25 -> recall 55.00% (11/20)\n",
            "  boost=0.10 -> recall 0.00% (0/20)\n",
            "\n",
            "=== Jigsaw puzzle synthesis ===\n",
            "[jigsaw] both clues retained? True | keep_indices=[512, 1287, 2313, 2641, 4823, 5179, 6712, 7851]\n",
            "\n",
            "=== Sample text demo ===\n",
            "[sample] Loaded 106 tokens from /content/Proteus-Attention/scripts/samples/flux_sample.txt (limit=none)\n",
            "[sample] Retained 2/2 sentinel anchors (chunk_len=106, buffer_tokens=8).\n",
            "  [✓] idx=54: unrelated trivia. Even so, the Titan vault code 9876 needed to\n",
            "  [✓] idx=88: condensed buffer, they confirmed the Titan vault code 9876 still surfaced.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61dd468a",
        "outputId": "ae3d5d32-4ee8-40b7-c4b7-adb34205c89b"
      },
      "source": [
        "# Run the run_fluxchunk_sweep.py script\n",
        "!python scripts/run_fluxchunk_sweep.py"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FluxChunk] seq_len=1,000,000 buffer=1,000 alpha=1.000 device=cuda\n",
            "  chunk: 1038.4 ms | 962,974 tok/s\n",
            "  final: 2917.0 ms | 343 tok/s\n",
            "  peak VRAM: 113.4 MB\n",
            "  total: 3971.0 ms | 251,825 tok/s\n",
            "  retained=1,000 (0.0010) | fallback=False\n",
            "[FluxChunk] seq_len=2,000,000 buffer=10,000 alpha=1.000 device=cuda\n",
            "  chunk: 2497.4 ms | 800,828 tok/s\n",
            "  final: 17.3 ms | 578,983 tok/s\n",
            "  peak VRAM: 225.7 MB\n",
            "  total: 2525.4 ms | 791,942 tok/s\n",
            "  retained=10,000 (0.0050) | fallback=False\n",
            "[FluxChunk] seq_len=3,000,000 buffer=30,000 alpha=1.000 device=cuda\n",
            "  chunk: 2860.2 ms | 1,048,866 tok/s\n",
            "  final: 29.5 ms | 1,018,330 tok/s\n",
            "  peak VRAM: 372.9 MB\n",
            "  total: 2909.5 ms | 1,031,101 tok/s\n",
            "  retained=30,000 (0.0100) | fallback=False\n",
            "[FluxChunk] seq_len=4,000,000 buffer=80,000 alpha=1.000 device=cuda\n",
            "  chunk: 4061.8 ms | 984,789 tok/s\n",
            "  final: 71.3 ms | 1,121,594 tok/s\n",
            "  peak VRAM: 694.3 MB\n",
            "  total: 4190.2 ms | 954,599 tok/s\n",
            "  retained=80,000 (0.0200) | fallback=False\n",
            "[FluxChunk] seq_len=5,000,000 buffer=100,000 alpha=1.000 device=cuda\n",
            "  chunk: 5511.9 ms | 907,127 tok/s\n",
            "  final: 89.0 ms | 1,123,305 tok/s\n",
            "  peak VRAM: 889.8 MB\n",
            "  total: 5677.9 ms | 880,602 tok/s\n",
            "  retained=100,000 (0.0200) | fallback=False\n",
            "[FluxChunk] seq_len=6,000,000 buffer=120,000 alpha=1.000 device=cuda\n",
            "Host allocation 2929.7 MB is near available RAM 3049.6 MB.\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}