{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Proteus Attention Playground (ASPA Edition)\n\nThis notebook walks through three short demos showcasing Adaptive Sparse Proto Attention (ASPA):\n\n1. **Baseline vs ASPA fine-tune** – train both models on Tiny Shakespeare and compare metrics.\n2. **Scaling ladder** – benchmark latency + memory from 4K up to 512K tokens with `scripts/tinytoy.py`.\n3. **Long-context storytelling** – stream a million-token prompt through the chunked shortlist pipeline and sample continuations.\n\nEach section is self-contained so you can skip ahead or rerun cells independently."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 0. Environment Setup\n\nRun the next cell on Colab to clone the repo (if needed) and install dependencies. On local machines you can skip the clone and just run the editable install."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\nif [ ! -d Proteus-Attention ]; then\n  git clone https://github.com/Zen-Sherbert/Proteus-Attention.git\nfi\ncd Proteus-Attention\npip install -q -e ."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Baseline vs ASPA Fine-Tune\n\nWe reuse `examples/aspa_train.py`, which trains both the ASPA model and the dense baseline on Tiny Shakespeare in a single run. The flags below keep things lightweight for Colab while still showing divergence in loss/latency.\n\n* `--epochs 2` keeps the run short.\n* `--block-size 2048` exercises medium contexts.\n* `--batch-size 4` fits comfortably in 16 GB GPUs.\n* `--run-label` tags the summary row for later inspection."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\ncd Proteus-Attention\npython examples/aspa_train.py \\n  --epochs 2 \\n  --block-size 2048 \\n  --batch-size 4 \\n  --d-model 512 \\n  --n-layer 4 \\n  --run-label playground-demo \\n  --no_compile \\n  --gen-tokens 80"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Inspect training summaries\n\nThe previous script writes a JSON summary under `examples/checkpoints/`. Use the helper below to load and display the latest entry."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\ncd Proteus-Attention\npython - <<'EOF'\nimport json\nfrom pathlib import Path\nroot = Path('examples/checkpoints')\nlogs = sorted(root.rglob('history.json'))\nif not logs:\n    print('No history files yet.')\nelse:\n    path = logs[-1]\n    data = json.loads(path.read_text())\n    print(f'Loaded {path}')\n    last = data[-1]\n    for k, v in last.items():\n        print(f\"{k:>16}: {v}\")\nEOF"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Plot training metrics\n\nIf `examples/aspa_train.py` wrote a `history.csv`, this cell plots train loss and validation perplexity for the most recent run."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nimport matplotlib.pyplot as plt\nimport pandas as pd\nroot = Path('Proteus-Attention/examples/checkpoints')\nfiles = sorted(root.rglob('history.csv'))\nif not files:\n    print('No history.csv files found yet.')\nelse:\n    df = pd.read_csv(files[-1])\n    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n    axes[0].plot(df['step'], df['loss'], label='train loss')\n    if 'val_loss' in df.columns:\n        axes[0].plot(df['step'], df['val_loss'], label='val loss')\n    axes[0].set_xlabel('step')\n    axes[0].set_ylabel('loss')\n    axes[0].legend()\n    axes[0].set_title('Train / Val Loss')\n    if 'val_ppl' in df.columns:\n        axes[1].plot(df['step'], df['val_ppl'], label='match ppl')\n    if 'val_ppl_fixed' in df.columns:\n        axes[1].plot(df['step'], df['val_ppl_fixed'], label='fixed ppl')\n    axes[1].set_xlabel('step')\n    axes[1].set_ylabel('perplexity')\n    axes[1].legend()\n    axes[1].set_title('Validation Perplexity')\n    fig.tight_layout()\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Scaling Ladder Benchmark\n\n`tinytoy.py` sweeps context lengths and reports latency/memory. The cell below targets a single GPU; adjust `--device` if you're on CPU. The `--report-dir` flag keeps JSON outputs under `reports/tinytoy` for later analysis."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\ncd Proteus-Attention\npython scripts/tinytoy.py \\n  --device auto \\n  --max-seq-count 8 \\n  --plot-path reports/tinytoy/playground.png \\n  --report-dir reports/tinytoy \\n  --no-save"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Display the generated plot (if matplotlib was available)\n\nIf matplotlib ran successfully, the plot saved under `reports/tinytoy/playground.png` will appear below."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\nfrom PIL import Image\npath = Path('Proteus-Attention/reports/tinytoy/playground.png')\nif path.is_file():\n    display(Image.open(path))\nelse:\n    print('Plot not found (likely matplotlib unavailable).')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Plot latency/memory summary\n\nLoads the newest `tinytoy_summary_*.json` from the reports directory and charts latency/memory vs sequence length for Standard vs ASPA runs."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport pandas as pd\nreports = Path('Proteus-Attention/reports/tinytoy')\nsummary_files = sorted(reports.glob('tinytoy_summary_*.json'))\nif not summary_files:\n    print('No tinytoy summary files found. Rerun Section 2 first.')\nelse:\n    data = json.loads(summary_files[-1].read_text())\n    runs = data.get('runs_summary', [])\n    if not runs:\n        print('Summary file missing runs_summary entries.')\n    else:\n        df = pd.DataFrame(runs)\n        fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n        for label, group in df.groupby('model'):\n            axes[0].plot(group['seq_len'], group['latency_ms'], marker='o', label=label)\n            axes[1].plot(group['seq_len'], group['mem_mb'], marker='o', label=label)\n        axes[0].set_xscale('log')\n        axes[1].set_xscale('log')\n        axes[0].set_xlabel('sequence length')\n        axes[1].set_xlabel('sequence length')\n        axes[0].set_ylabel('latency (ms)')\n        axes[1].set_ylabel('memory (MB)')\n        axes[0].set_title('Latency vs Seq Len')\n        axes[1].set_title('Peak Memory vs Seq Len')\n        for ax in axes:\n            ax.legend()\n            ax.grid(True, which='both', ls='--', alpha=0.3)\n        fig.tight_layout()\n        plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Long-Context Storytelling with Chunked Shortlist\n\nThis step streams a ~1 M token prompt through the chunked shortlist runner (`scripts/chunked_shortlist.py`).\n\n* `--seq-len` defines the total tokens to simulate.\n* `--chunk-len` controls streaming windows (kept ≤65 K for ROCm).\n* `--alpha 1.0` forces the linear shortlist mode.\n\nThe script prints shortlist diagnostics and samples a short continuation with Top‑A stats."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\ncd Proteus-Attention\npython scripts/chunked_shortlist.py \\n  --seq-len 1048576 \\n  --chunk-len 65536 \\n  --chunk-ratio 0.05 \\n  --chunk-budget 4096 \\n  --alpha 1.0 \\n  --sample-tokens 120"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Next Steps\n\n* Tweak `examples/aspa_train.py` flags to match your dataset or hardware.\n* Run `scripts/chunked_shortlist_tests.py` for synthetic stress tests.\n* Export the `reports/tinytoy/*.json` files to your own dashboards or add new stages to the notebook.\n\nHappy experimenting!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}