{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0ovbZd6KSfN"
      },
      "source": [
        "# Proteus Attention Walkthrough\n",
        "\n",
        "This is a concise, tour of Adaptive Sparse Proto Attention (ASPA). Each section runs a single command, prints its telemetry directly in the notebook, and stops. It takes a while due to the nature of the tests. Run them and wait if you wish, or use the provided metrics. Ensure T4 or better is enabled.\n",
        "\n",
        "Sections\n",
        "1. Environment & prerequisites\n",
        "2. Tiny Shakespeare duel (dense vs ASPA)\n",
        "3. Context-length sweep (`tinytoy.py`)\n",
        "4. Chunked shortlist stream"
      ],
      "id": "v0ovbZd6KSfN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TjsZ1pzKSfP"
      },
      "source": [
        "## 0. Environment & prerequisites\n",
        "Run this to clone the repo and install the package. Safe to rerun; it becomes a no-op after the first install."
      ],
      "id": "6TjsZ1pzKSfP"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3B6pIX4KSfP",
        "outputId": "097ac5ec-e2f6-42cf-a45e-8e364a8c0b0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Cloning into 'Proteus-Attention'...\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "if [ ! -d Proteus-Attention ]; then\n",
        "  git clone https://github.com/Zen-Sherbert/Proteus-Attention.git\n",
        "fi\n",
        "cd Proteus-Attention\n",
        "pip install -q -e .\n"
      ],
      "id": "P3B6pIX4KSfP"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_ZbYNoZKSfP"
      },
      "source": [
        "## 1. Tiny Shakespeare duel\n",
        "Trains both the ASPA stack and the dense baseline using the default Tiny Shakespeare config. All metrics, router stats, and sample generations show up right below the cell. This takes a hot minute. Suggested that you run this when you have time."
      ],
      "id": "-_ZbYNoZKSfP"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r01PTrKDKSfQ",
        "outputId": "a17c6a0d-ba85-4617-f846-e244cf002110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is terminated.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "cd Proteus-Attention\n",
        "python src/proteus_attention/examples/aspa_train.py   --epochs 2   --batch_size 1   --block_size 2048   --d_model 512   --n_layer 4   --lr 3e-4   --target_density 0.28   --density_tol 0.05   --min_active_heads 2   --max_active_heads 4   --token_sparse   --token_keep_ratio 0.85   --token_keep_min 8   --token_keep_guard 8   --gen_tokens 80"
      ],
      "id": "r01PTrKDKSfQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWW_vtRlKSfQ"
      },
      "source": [
        "## 2. Context-length sweep\n",
        "Runs the tinytoy sweep, showing direct stats over standard.\n",
        "\n",
        "Take it with a grain of salt, as it was mainly meant to showcase structural architecture.\n",
        "\n",
        "There are many elements to look at regardless when it does pertain to the structure. Alpha is our control knob for sparsity and computational complexity. Shifting between quadratic and linear when you push the value of alpha higher.\n",
        "\n",
        "This test takes ~2-3 minutes. Shortlist chunking is currently bork'd in tinytoy due to a refactor. Repairs are on the way.\n"
      ],
      "id": "bWW_vtRlKSfQ"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMOR0XnUKSfQ",
        "outputId": "4c346a35-e62f-4d12-bfbf-4669434d54b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Config] SDPA fast-path disabled; benchmarking Shortlist kernels only.\n",
            "--- Starting Attention Benchmark ---\n",
            "Device: cuda\n",
            "Sequence lengths: [128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576]\n",
            "\n",
            "[Probe] Standard testing seq_len=128 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=256 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=512 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=1024 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=2048 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=4096 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=8192 batch=1 runs=1\n",
            "[Probe] Standard testing seq_len=16384 batch=1 runs=1\n",
            "[Probe] Standard testing seq_len=32768 batch=1 runs=1\n",
            "[Probe] Standard failed at sequence length 32768: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.44 GiB is free. Process 102090 has 308.00 MiB memory in use. Of the allocated memory 138.13 MiB is allocated by PyTorch, and 43.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "[Probe] Standard safe up to sequence length 16384 on this device.\n",
            "[Probe] ASPA testing seq_len=128 batch=4 runs=3\n",
            "[Probe] ASPA testing seq_len=256 batch=4 runs=3\n",
            "[Probe] ASPA testing seq_len=512 batch=4 runs=3\n",
            "[Probe] ASPA testing seq_len=1024 batch=4 runs=3\n",
            "[Probe] ASPA testing seq_len=2048 batch=4 runs=3\n",
            "[Probe] ASPA testing seq_len=4096 batch=4 runs=3\n",
            "[Probe] ASPA testing seq_len=8192 batch=1 runs=1\n",
            "[Probe] ASPA testing seq_len=16384 batch=1 runs=1\n",
            "[Probe] ASPA testing seq_len=32768 batch=1 runs=1\n",
            "[Probe] ASPA testing seq_len=65536 batch=1 runs=1\n",
            "[Probe] ASPA testing seq_len=131072 batch=1 runs=1\n",
            "[Probe] ASPA testing seq_len=262144 batch=1 runs=1\n",
            "[Probe] ASPA testing seq_len=524288 batch=1 runs=1\n",
            "[Probe] ASPA testing seq_len=1048576 batch=1 runs=1\n",
            "[Probe] ASPA failed at sequence length 1048576: CUDA out of memory. Tried to allocate 6.60 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.95 GiB is free. Process 102090 has 9.79 GiB memory in use. Of the allocated memory 9.59 GiB is allocated by PyTorch, and 37.46 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "[Probe] ASPA safe up to sequence length 524288 on this device.\n",
            "Seq Len    | Model                    | Latency (ms)    | Seq/s        | Tok/s           | Memory (MB)     | Active K   | Tokens     | Alpha   | Mode             | Backend                             \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[Warmup] Priming shortlist kernel at seq_len=8192...\n",
            "128        | Standard Attention       | 0.46            | 8737.51      | 1118401.15      | 14.15           | -          | -          | -       | -                | torch.nn                            \n",
            "128        | ASPA (BF16)              | 5.40            | 741.25       | 94880.20        | 18.85           | 8          | 1.00       | 0.00    | dense_alpha      | triton (max_rows=128, density=0.33, unique=32, tokens=1.00, proto_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=128 backend=triton\n",
            "128        | ASPA (INT8)              | 5.52            | 724.30       | 92710.55        | 18.85           | 8          | 1.00       | 0.00    | dense            | triton (max_rows=128, density=0.34, unique=32, tokens=1.00, proto_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=128 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "256        | Standard Attention       | 0.94            | 4251.68      | 1088430.35      | 18.65           | -          | -          | -       | -                | torch.nn                            \n",
            "256        | ASPA (BF16)              | 6.20            | 645.34       | 165206.04       | 27.29           | 8          | 1.00       | 0.00    | dense_alpha      | triton (max_rows=256, density=0.32, unique=32, tokens=1.00, proto_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=256 backend=triton\n",
            "256        | ASPA (INT8)              | 4.62            | 866.39       | 221794.98       | 27.29           | 8          | 1.00       | 0.00    | dense            | triton (max_rows=256, density=0.32, unique=32, tokens=1.00, proto_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=256 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "512        | Standard Attention       | 2.66            | 1503.35      | 769717.49       | 35.65           | -          | -          | -       | -                | torch.nn                            \n",
            "512        | ASPA (BF16)              | 7.71            | 518.82       | 265636.45       | 44.33           | 7          | 0.80       | 0.03    | dense_alpha      | triton (max_rows=512, density=0.22, unique=32, tokens=0.80, proto_blend=0.00, alpha=0.03)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=512 backend=triton\n",
            "512        | ASPA (INT8)              | 21.24           | 188.33       | 96427.13        | 244.97          | 7          | 0.80       | 0.03    | shortlist        | triton (max_rows=114, density=0.22, unique=32, tokens=0.80, proto_blend=0.00, alpha=0.03)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=512 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "1024       | Standard Attention       | 7.97            | 501.59       | 513629.93       | 93.83           | -          | -          | -       | -                | torch.nn                            \n",
            "1024       | ASPA (BF16)              | 16.70           | 239.46       | 245211.30       | 79.36           | 7          | 0.80       | 0.10    | dense_alpha      | triton (max_rows=1024, density=0.20, unique=32, tokens=0.80, proto_blend=0.00, alpha=0.10)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=1024 backend=triton\n",
            "1024       | ASPA (INT8)              | 43.39           | 92.18        | 94395.01        | 372.65          | 7          | 0.80       | 0.10    | shortlist        | triton (max_rows=204, density=0.20, unique=32, tokens=0.80, proto_blend=0.00, alpha=0.10)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=1024 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "2048       | Standard Attention       | 25.19           | 158.82       | 325266.43       | 306.00          | -          | -          | -       | -                | torch.nn                            \n",
            "2048       | ASPA (BF16)              | 52.46           | 76.24        | 156144.92       | 152.43          | 5          | 0.60       | 0.23    | dense_alpha      | triton (max_rows=2048, density=0.11, unique=32, tokens=0.60, proto_blend=0.00, alpha=0.23)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=2048 backend=triton\n",
            "2048       | ASPA (INT8)              | 56.55           | 70.73        | 144854.44       | 511.48          | 5          | 0.60       | 0.23    | shortlist        | triton (max_rows=227, density=0.11, unique=32, tokens=0.60, proto_blend=0.00, alpha=0.23)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=2048 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "4096       | Standard Attention       | 92.79           | 43.11        | 176576.38       | 1114.03         | -          | -          | -       | -                | torch.nn                            \n",
            "4096       | ASPA (BF16)              | 145.98          | 27.40        | 112235.49       | 832.44          | 4          | 0.52       | 0.48    | shortlist        | triton (max_rows=317, density=0.08, unique=32, tokens=0.52, proto_blend=0.00, alpha=0.48)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=4096 backend=triton\n",
            "4096       | ASPA (INT8)              | 92.77           | 43.12        | 176613.57       | 896.85          | 4          | 0.52       | 0.48    | shortlist        | triton (max_rows=306, density=0.07, unique=32, tokens=0.52, proto_blend=0.00, alpha=0.48)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=4096 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "8192       | Standard Attention       | 80.91           | 12.36        | 101253.32       | 1098.17         | -          | -          | -       | -                | torch.nn                            \n",
            "8192       | ASPA (BF16)              | 33.17           | 30.15        | 246973.29       | 451.89          | 2          | 0.23       | 1.00    | shortlist        | triton (max_rows=149, density=0.02, unique=32, tokens=0.23, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=8192 backend=triton\n",
            "8192       | ASPA (INT8)              | 26.41           | 37.86        | 310183.90       | 462.59          | 2          | 0.23       | 1.00    | shortlist        | triton (max_rows=149, density=0.02, unique=32, tokens=0.23, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=8192 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "16384      | Standard Attention       | 305.60          | 3.27         | 53612.82        | 4209.71         | -          | -          | -       | -                | torch.nn                            \n",
            "16384      | ASPA (BF16)              | 55.74           | 17.94        | 293955.23       | 621.25          | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=265, density=0.02, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=16384 backend=triton\n",
            "16384      | ASPA (INT8)              | 45.84           | 21.81        | 357382.66       | 643.45          | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=260, density=0.02, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=16384 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[Notice] Standard attention hit its limit at seq_len=32768 (last safe ~16384).\n",
            "32768      | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -       | -                | std-limit                           \n",
            "32768      | ASPA (BF16)              | 110.59          | 9.04         | 296294.88       | 968.25          | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=535, density=0.02, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=32768 backend=triton\n",
            "32768      | ASPA (INT8)              | 90.13           | 11.10        | 363579.66       | 1016.14         | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=499, density=0.02, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=32768 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "65536      | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -       | -                | std-limit                           \n",
            "65536      | ASPA (BF16)              | 218.72          | 4.57         | 299631.17       | 1557.88         | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=1756, density=0.03, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=65536 backend=triton\n",
            "65536      | ASPA (INT8)              | 178.84          | 5.59         | 366444.95       | 1955.54         | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=1449, density=0.02, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=65536 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "131072     | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -       | -                | std-limit                           \n",
            "131072     | ASPA (BF16)              | 443.65          | 2.25         | 295441.43       | 3045.68         | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=2890, density=0.02, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=131072 backend=triton\n",
            "131072     | ASPA (INT8)              | 365.16          | 2.74         | 358945.08       | 3878.45         | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=2526, density=0.02, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=131072 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "262144     | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -       | -                | std-limit                           \n",
            "262144     | ASPA (BF16)              | 901.80          | 1.11         | 290688.76       | 6055.11         | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=12307, density=0.05, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=262144 backend=triton\n",
            "262144     | ASPA (INT8)              | 746.92          | 1.34         | 350967.78       | 7722.81         | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=9705, density=0.04, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (INT8) seq_len=262144 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "524288     | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -       | -                | std-limit                           \n",
            "524288     | ASPA (BF16)              | 1831.75         | 0.55         | 286222.34       | 12079.09        | 2          | 0.21       | 1.00    | shortlist        | triton (max_rows=23761, density=0.05, unique=32, tokens=0.21, proto_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured ASPA (BF16) seq_len=524288 backend=triton\n",
            "[Notice] ASPA (INT8) hit its limit at seq_len=524288 (last safe ~262144).\n",
            "524288     | ASPA (INT8)              | OOM             | -            | -               | -               | -          | -          | 1.00    | auto             | aspa-limit                          \n",
            "524288     | ASPA (INT8)              | OOM             | -            | -               | -               | -          | -          | -       | -                | aspa-limit                          \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Summary] Standard safe length: 16384; ASPA safe length: 524288; Chunked Shortlist safe length: disabled.\n",
            "[Plot] Saved latency plot to reports/tinytoy/tinytoy_summary_20251109-025435.png\n",
            "\n",
            "[Summary] Detailed JSON report saved to reports/tinytoy/tinytoy_summary_20251109-025435.json\n",
            "[Summary] Captured 25 AutoLog entries.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "cd Proteus-Attention\n",
        "python src/proteus_attention/kernels/tinytoy.py\n"
      ],
      "id": "fMOR0XnUKSfQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUNemkFTKSfQ"
      },
      "source": [
        "## 3. Chunked shortlist stream\n",
        "Streams a million token synthetic prompt through the shortlist pipeline. Diagnostics (chunk stats, Top-A agreement, backend info) appear inline.\n",
        "\n",
        "There is an error with Top-A I believe, which is being worked on.\n",
        "\n",
        "The main thing to look at here is that chunking is working, and that it can identify storage limits. If overflow happens, it should correctly shunt it to disk storage to prevent collapse. This is still highly experimental and the quality of anything actually used from chunks has yet to be tested.\n",
        "\n",
        "The mechanics are sound for the most part, but until I actually test the true, live system, then its primarily speculation."
      ],
      "id": "pUNemkFTKSfQ"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWPCc-N0KSfR",
        "outputId": "6f18afa3-bb3f-4aa5-be94-185d0904f839"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-11-09 02:34:50,615 | INFO | proteus_attention.tools.chunked_shortlist | Host staging: cpu (limit=10,272,145,408B, required=2,147,483,648B)\n",
            "2025-11-09 02:37:22,320 | INFO | proteus_attention.tools.chunked_shortlist | Chunked Shortlist retained 32768/1048576 tokens (0.031) across 16 chunks\n",
            "2025-11-09 02:37:22,321 | WARNING | proteus_attention.tools.chunked_shortlist | Router importance unavailable; used norm-based fallback scoring.\n",
            "2025-11-09 02:37:22,517 | INFO | root | Chunked Shortlist complete: retained 32768/1048576 tokens (3.12%) on cpu\n",
            "2025-11-09 02:37:22,517 | INFO | root | Chunk streaming latency: 134209.5 ms\n",
            "2025-11-09 02:37:22,517 | INFO | root | Chunk throughput: 7812.98 tok/s\n",
            "2025-11-09 02:37:22,517 | INFO | root | Final pass latency: 8368.0 ms\n",
            "2025-11-09 02:37:22,517 | INFO | root | Final pass throughput: 3915.88 tok/s\n",
            "2025-11-09 02:37:22,517 | INFO | root | Overall throughput: 7349.89 tok/s\n",
            "2025-11-09 02:37:22,517 | INFO | root | Storage mode: cpu\n",
            "2025-11-09 02:37:22,517 | INFO | root | Storage reasoning: limit=10,272,145,408B, required=2,147,483,648B\n",
            "2025-11-09 02:37:22,517 | INFO | root | Host requirement: 2048.0 MB\n",
            "2025-11-09 02:37:22,517 | INFO | root | Host allocated: 2048.0 MB\n",
            "2025-11-09 02:37:22,518 | INFO | root | Host limit: 9796.3 MB\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "cd Proteus-Attention\n",
        "python scripts/chunked_shortlist.py   --seq-len 1048576   --d-model 512   --chunk-len 65536   --buffer-tokens 32768   --per-chunk-budget 4096   --chunk-sparse-ratio 0.05   --final-sparse-ratio 0.5   --shortlist-alpha 1.0   --nucleus-top-p 0.9   --heads 8   --device cpu   --report-latency"
      ],
      "id": "FWPCc-N0KSfR"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJ1Ksg1lKSfR"
      },
      "source": [
        "# Breakdown\n",
        "\n",
        "There is a lot going on here and its difficult to showcase some features without explination. So I will do my best to describe what is going on.\n",
        "\n",
        "There are some fundamental differences with what is going on here and what is being done currently in the field.\n",
        "\n",
        "None of these concepts are new in any way. I have not discovered some novel system no one has thought of and I refute any claims to that.\n",
        "\n",
        "What I am claiming is novel architecture.\n",
        "\n",
        "So to try help the headache, I will describe some of the things I have here as a sort of glossery.\n",
        "\n",
        "**Adaptive Sparse Proto Attention (ASPA)**\n",
        "\n",
        "*Prototypes*\n",
        "\n",
        "So this is a confusing and clinical term, but I will describe their function in this system regardless.\n",
        "\n",
        "By taking tokens and essentially fingerprinting them and taking their semantic values, we add that to the prototype and update it, creating a centroid that is an average of all the tokens it has seen. More recent tokens take are weighted more than older ones, creating a moving average.  So when it sees tokens close to its value, it acts as a lure to the token.\n",
        "\n",
        "*Decoupled Gating*\n",
        "\n",
        "This is more of an accidental architectual choice than some strategic calculation. This turned out to be pretty awesome though when you look into it.\n",
        "\n",
        "So decoupling gates alone wasn't what was awesome. It was actually when you assign more gates to heads, that it became interesting. It creates a buffer between the router and heads, which essentially lets you do some weird things. This is where my sparsity is kind of strange, as it doesnt rely on patterns or anything.\n",
        "\n",
        "The gates hold the protypes, and each head has more than one assigned to it. Creating a granulated effect on how they take tokens.\n",
        "\n",
        "Another interesting concept with this is actually what this does. The router no longer cares whats behind the gates. It only cares about the gates themselves. This means you can dynamically clone, prune, merge, etc.\n",
        "\n",
        "Anything behind these gates can be changed. When applied to the MLP, it gets even better.\n",
        "\n",
        "*Dynamic Heads*\n",
        "\n",
        "I touched on it in the decoupled gates section, and there isnt a focus on it yet, though the system naturally supports it.\n",
        "\n",
        "You can choose the amount of heads you want, say you want 64. You can assign that and it will work just fine. Increase that to 2000 heads, and it will also run that. Not all at once, but in an intelligent sparse mode. So you can have a really talented attention layer. Thats not to say that its not without issues, because it brings the question of actual intelligent assignment. How do we know its evenly splitting tokens across different heads?\n",
        "\n",
        "One interesting thing is, that we can infact add more heads later on. Since we assign gates to heads, and we use a many to one assignment ratio, we can dynamically add more heads as long as there are gates to support them.\n",
        "\n",
        "*Double Sparcity*\n",
        "\n",
        "By leaning on the prototypes, we can actually create a salience based token pruning system. We can identify tokens that the model thinks are important due to its learned protypes, and when coupled with head sparcity, we can push speed and long context while keeping our memory footprint low.\n",
        "\n",
        "This isnt to say that this is a perfect system. It can easily miss tokens if not properly trained, and it can lose context quality because of this. So this still have many areas that can be improved on or changed entirely.\n",
        "\n",
        "*RoPE + Prototypes*\n",
        "\n",
        "A unique synergy that has been observed is actally the interaction of using RoPE for positional data, and Prototying for saliance data. If it has seen a concept ages ago in a document, it can actually bridge two concepts across a large document as if they were connected in the same area. This is powerful for long context, but its not a catch all.\n",
        "\n",
        "*Top-A*\n",
        "\n",
        "Didnt know what to name this, so if it is fundamentally wrong, please say so.\n",
        "\n",
        "Top-A is an agreement filter. We use it in conjunction with Top-P. This create a double filter effect where Top-P chooses how many tokens, and Top-A uses cosine simularity between heads to decide what tokens to actully use. Theoretically is a synamic correction for uncertainty, but its still dependants on training, and quality. So the most I can say is that its an optional experimental filter.\n",
        "\n",
        "*KV Banking*\n",
        "\n",
        "Again this is another concept I wasnt sure about its naming convention. Its similar to a KV cache, but its more robust in its use and ability.\n",
        "\n",
        "The KV Bank keeps several caches. These are usually associated with specific heads. It acts as a shortlist of concepts it has seen and known. Its dynamic in the sense that what is in it, is not read-only. If it finds a better representation, it will replace the old one. So if you think of a KV Cache, it would be good to say it was a notebook. If you look at KV Bank, it would be better to call it sticky notes.  \n",
        "So KV Cache with less steps and more selective memory and per head.\n",
        "\n",
        "*Shortlist Chunking*\n",
        "\n",
        "This is what we use to extend our reach to massive distances.\n",
        "\n",
        "Instead of processing an entire 10M token document, we chunk it into sections. Feeding that to our model, we can then use some of its unique features to piece together things. Its not 100% effective and it wont be able to summarize entire sections, but for precise information, it would work well.\n",
        "\n",
        "What happens is that as the model looks over the chunk, it collects data relating to the prompt. So if the user is asking about the best way to cook a chicken caprese and the context is a mass of jumbled recipes, reciepts, random books, automotive manuals and dog food ingrediants, It would be able to search each chunk and find the chicken caprese recipe, while also finding other bits of information relating to the concept. Store it in the buffer and send that back to the model.\n",
        "\n",
        "You would think that it would be a jumbled mess of random exerpts of words, but there is contextual meaning, semantic meaning, positional data, etc, that we as humans wouldnt understand. So the model can generally answer a question.\n",
        "\n",
        "This is not better than anything else out there by any means, its entirely untested outside of my own experiments. What it aims to be is a good alternative to extreme long context that the standalone model can perform on its own.\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "ASPA is a highly ambitious project aimed at making the attention smarter and better at handling long context.\n",
        "\n",
        "It is not a drop in replacement for attention, and it still has a huge distance to cover.\n",
        "\n",
        "There are trade offs, massive ones that come with this architecture, that can be engineered around, but not out of entirely. The overhead and complexity is a massive issue, and I'm doing my best to reign it all in and create per system adaptation at cheap upfront computational costs.\n",
        "\n",
        "The net positives arguably can outweigh the negatives, especially in longer context.\n",
        "\n",
        "Thank you for visiting and giving this a read.\n",
        "If you have corrections, or if you think there are fundamental issues with the project that should be addessed, I would appreciate if you could bring it up so I can address it and fix it.\n",
        "\n",
        "If there are bold assumptions in this and they make you upset, please bring it up and I will either explain why it is said that way, or fix it."
      ],
      "id": "QJ1Ksg1lKSfR"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
