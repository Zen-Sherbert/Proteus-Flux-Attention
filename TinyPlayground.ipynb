{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyObKMRd9c8BwTMVJq4g7Y1v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zen-Sherbert/Proteus-Attention/blob/main/TinyPlayground.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Welcome to the Proteus Attention Playground!\n",
        "\n",
        "This notebook was made because I'm bad at explaining things, and it's better to show, not tell.\n",
        "\n",
        "A little Q&A before we start:\n",
        "\n",
        "**What is this project?**\n",
        "\n",
        "A: Proteus is an attention architecture that gives AI a new superpower: **Selective Memory.**\n",
        "\n",
        "Instead of treating every word in a vast context equally, it learns the concept of **Salience**, the art of identifying what is truly important. It then curates a perfect, high-fidelity \"highlight reel\" of the entire context, allowing it to reason over millions of tokens with a tiny, fixed memory footprint. It's not just a bigger context window; it's a smarter one.\n",
        "\n",
        "**How is this different from RAG, Mamba, or anything else out there?**\n",
        "\n",
        "A: Proteus isn't just one new idea; it's a synthesis of three systems that work together to create a new paradigm.\n",
        "\n",
        "---\n",
        "\n",
        "### **Sys 1: The CPU or Brain (DNA & Hybrid Routing)**\n",
        "\n",
        "My philosophy is that tokens aren't just data. They have learnable values and metrics. They are the food of an AI and should be treated with more respect.\n",
        "\n",
        "*   **The System:** I created a salience-gating system called **DNA**. Think of it as giving the AI \"taste.\" As the model trains, its attention \"gates\" evolve and mutate, learning to recognize the \"flavor\" of important concepts. A gate might learn to prefer tokens related to legal language, while another learns to prefer tokens related to Python code.\n",
        "*   **The Result:** This creates a **Hybrid Router**. A standard router acts as a backup, but the primary driver is the DNA's \"instinct.\" It actively draws in tokens that match its learned preferences. This is the **\"what\"**, it tells the system what information is worth paying attention to.\n",
        "\n",
        "---\n",
        "\n",
        "### **Sys 2: The Chassis (Radical Sparsity & The Alpha Slider)**\n",
        "\n",
        "Why should a model have eight heads? Or sixteen? Why not 20,000? I am not joking.\n",
        "\n",
        "*   **The System:** Proteus is designed to handle a massive number of specialized attention heads. Because you can't use all 20,000 at once, the system is **inherently sparse**, it only activates a tiny fraction of heads for any given token. This radical sparsity is what allowed me to build a single, custom kernel that is a master of all trades.\n",
        "*   **The Result:** A single parameter, the **Alpha Slider**, acts as the \"gear shift\" for the entire architecture.\n",
        "    *   At `alpha=0.0`, it uses a careful, high-fidelity sparse mode for short contexts.\n",
        "    *   As you increase `alpha` towards `1.0`, it smoothly transitions into a linear-time \"bullet train\" for extreme contexts.\n",
        "\n",
        "---\n",
        "\n",
        "### **Sys 3: The Data (Chunking & Coherent Recall)**\n",
        "\n",
        "Ever play Minecraft and watch the world load in, chunk by chunk?\n",
        "\n",
        "*   **The System:** To handle a context of 100 million tokens, you don't load it all at once. You break that \"chocolate bar\" into even chunks. The system streams these chunks one by one, using its DNA to identify the most salient \"highlight\" tokens from each. These champions are placed into a small, fixed-size buffer in VRAM.\n",
        "*   **The Result: An \"Impossible\" Memory.** This is where the magic happens, and it's natural to be skeptical. You'd think the buffer would be a jumbled, incoherent mess. You'd be wrong for two reasons:\n",
        "    1.  **DNA is the \"What\":** The salience system ensures that the *right* information (the recipe, the apples, the pie) makes it into the buffer.\n",
        "    2.  **RoPE is the \"Where\":** Rotary Position Embeddings provide an unbreakable, absolute sense of position. The system doesn't just know *what* was said; it knows *where* it was said in the original document.\n",
        "\n",
        "---\n",
        "\n",
        "### **The Synthesis: Teleportation**\n",
        "\n",
        "When you combine these three systems, you get something that feels like science fiction.\n",
        "\n",
        "The system can be at the end of a 10-million-token document and, through its salience-based memory, create a direct, instantaneous informational link to a single, critical sentence from the very beginning.\n",
        "\n",
        "It's like an **Einstein-Rosen Bridge** for context. It connects two distant points in the sequence, making them appear right next to each other for the final reasoning pass. It is a weird, creepy, and unbelievably powerful mechanic.\n",
        "\n",
        "Wild, I know. Now, let's prove it.\n"
      ],
      "metadata": {
        "id": "DcDk4cbcbjLd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_K-gpLELABM",
        "outputId": "1218b079-9ae4-4926-8dc0-8207ae139fb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Proteus-Attention'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (108/108), done.\u001b[K\n",
            "remote: Total 136 (delta 43), reused 91 (delta 19), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (136/136), 164.82 KiB | 1.28 MiB/s, done.\n",
            "Resolving deltas: 100% (43/43), done.\n",
            "/content/Proteus-Attention\n",
            "Repository files:\n",
            "examples/  MANIFEST.in\t   README.md  src/    TinyPlayground.ipynb\n",
            "LICENSE    pyproject.toml  scripts/   tests/  tinytoy_run.txt\n"
          ]
        }
      ],
      "source": [
        "# 1. Clone the repo, probably a no brainer.\n",
        "!git clone https://github.com/Zen-Sherbert/Proteus-Attention.git\n",
        "\n",
        "%cd Proteus-Attention\n",
        "\n",
        "print(\"Repository files:\")\n",
        "!ls -F"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5da73279",
        "outputId": "b9fafd9e-dee4-4232-9503-4a7bd7fc99fc"
      },
      "source": [
        "# Install the proteus_attention package (It'll kind of run without it, but thats like having no oil in a car)\n",
        "!pip install ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/Proteus-Attention\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=2.1 in /usr/local/lib/python3.12/dist-packages (from proteus-attention==0.1.0) (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.1->proteus-attention==0.1.0) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.1->proteus-attention==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.1->proteus-attention==0.1.0) (3.0.3)\n",
            "Building wheels for collected packages: proteus-attention\n",
            "  Building wheel for proteus-attention (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for proteus-attention: filename=proteus_attention-0.1.0-py3-none-any.whl size=77301 sha256=f373ff72df602fac07cb516c9b8d42d0a5640331b86c731601e15d96a457801c\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/38/b5/a9e731a0381ba75a59307d7a0512b8a0ac1e70c603ff48dc4f\n",
            "Successfully built proteus-attention\n",
            "Installing collected packages: proteus-attention\n",
            "Successfully installed proteus-attention-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cd9c01c",
        "outputId": "d1dadcb9-43f4-4b44-ea16-a5f845fba704"
      },
      "source": [
        "# 4. This is the TinyToy benchmark. Make fun of it please. I hate it and it's been the source of constant headaches.\n",
        "# Tiny Toy Compares a standard dense model against some of the different modes of my system at different context lengths. There are some CLI arguments, but they are a headache right now.\n",
        "!python src/proteus_attention/kernels/tinytoy.py --enable-flux-chunk"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Config] SDPA fast-path disabled; benchmarking Flux kernels only.\n",
            "--- Starting Attention Benchmark ---\n",
            "Device: cuda\n",
            "Sequence lengths: [128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576]\n",
            "\n",
            "[Probe] Standard testing seq_len=128 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=256 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=512 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=1024 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=2048 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=4096 batch=4 runs=3\n",
            "[Probe] Standard testing seq_len=8192 batch=1 runs=1\n",
            "[Probe] Standard testing seq_len=16384 batch=1 runs=1\n",
            "[Probe] Standard testing seq_len=32768 batch=1 runs=1\n",
            "[Probe] Standard failed at sequence length 32768: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 14.74 GiB of which 14.44 GiB is free. Process 10366 has 308.00 MiB memory in use. Of the allocated memory 138.13 MiB is allocated by PyTorch, and 43.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "[Probe] Standard safe up to sequence length 16384 on this device.\n",
            "[Probe] DMoAH testing seq_len=128 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=256 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=512 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=1024 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=2048 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=4096 batch=4 runs=3\n",
            "[Probe] DMoAH testing seq_len=8192 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=16384 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=32768 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=65536 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=131072 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=262144 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=524288 batch=1 runs=1\n",
            "[Probe] DMoAH testing seq_len=1048576 batch=1 runs=1\n",
            "[Probe] DMoAH failed at sequence length 1048576: CUDA out of memory. Tried to allocate 6.60 GiB. GPU 0 has a total capacity of 14.74 GiB of which 4.91 GiB is free. Process 10366 has 9.82 GiB memory in use. Of the allocated memory 9.59 GiB is allocated by PyTorch, and 37.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "[Probe] DMoAH safe up to sequence length 524288 on this device.\n",
            "[Probe] FluxChunk testing seq_len=8192\n",
            "[Probe] FluxChunk testing seq_len=16384\n",
            "[Probe] FluxChunk testing seq_len=32768\n",
            "[Probe] FluxChunk testing seq_len=65536\n",
            "[Probe] FluxChunk testing seq_len=131072\n",
            "[Probe] FluxChunk testing seq_len=262144\n",
            "[Probe] FluxChunk testing seq_len=524288\n",
            "[Probe] FluxChunk testing seq_len=1048576\n",
            "[Probe] FluxChunk safe up to sequence length 1048576 on this device.\n",
            "Seq Len    | Model                    | Latency (ms)    | Seq/s        | Tok/s           | Memory (MB)     | Active K   | Tokens     | Mode       | Backend                             \n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[Warmup] Priming flux kernel at seq_len=8192...\n",
            "128        | Standard Attention       | 0.40            | 10027.34     | 1283500.10      | 14.13           | -          | -          | -          | torch.nn                            \n",
            "128        | DMoAH (BF16)             | 3.94            | 1015.13      | 129937.08       | 18.33           | 8          | 1.00       | dense      | triton (max_rows=128, density=0.34, unique=32, tokens=1.00, dna_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=128 backend=triton\n",
            "128        | DMoAH (INT8)             | 4.00            | 1000.30      | 128038.60       | 18.33           | 8          | 1.00       | dense      | triton (max_rows=128, density=0.34, unique=32, tokens=1.00, dna_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=128 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "256        | Standard Attention       | 0.91            | 4389.02      | 1123588.01      | 18.63           | -          | -          | -          | torch.nn                            \n",
            "256        | DMoAH (BF16)             | 4.44            | 899.89       | 230371.40       | 26.27           | 8          | 1.00       | dense      | triton (max_rows=256, density=0.31, unique=32, tokens=1.00, dna_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=256 backend=triton\n",
            "256        | DMoAH (INT8)             | 4.55            | 879.43       | 225134.65       | 26.27           | 8          | 1.00       | dense      | triton (max_rows=256, density=0.34, unique=32, tokens=1.00, dna_blend=0.00, alpha=0.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=256 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "512        | Standard Attention       | 2.66            | 1501.48      | 768755.97       | 35.63           | -          | -          | -          | torch.nn                            \n",
            "512        | DMoAH (BF16)             | 6.23            | 641.94       | 328673.81       | 50.61           | 7          | 0.80       | sparse     | triton (max_rows=112, density=0.22, unique=32, tokens=0.80, dna_blend=0.00, alpha=0.03)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=512 backend=triton\n",
            "512        | DMoAH (INT8)             | 6.86            | 583.44       | 298719.94       | 56.91           | 7          | 0.80       | sparse     | triton (max_rows=113, density=0.22, unique=32, int8, tokens=0.80, dna_blend=0.00, alpha=0.03)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=512 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "1024       | Standard Attention       | 7.74            | 516.79       | 529190.01       | 93.81           | -          | -          | -          | torch.nn                            \n",
            "1024       | DMoAH (BF16)             | 9.37            | 426.70       | 436939.26       | 91.94           | 7          | 0.80       | sparse     | triton (max_rows=210, density=0.21, unique=32, tokens=0.80, dna_blend=0.00, alpha=0.10)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=1024 backend=triton\n",
            "1024       | DMoAH (INT8)             | 10.79           | 370.74       | 379639.85       | 104.99          | 7          | 0.80       | sparse     | triton (max_rows=207, density=0.20, unique=32, int8, tokens=0.80, dna_blend=0.00, alpha=0.10)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=1024 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "2048       | Standard Attention       | 24.95           | 160.29       | 328272.42       | 305.98          | -          | -          | -          | torch.nn                            \n",
            "2048       | DMoAH (BF16)             | 16.49           | 242.53       | 496710.10       | 177.46          | 6          | 0.60       | sparse     | triton (max_rows=262, density=0.13, unique=32, tokens=0.60, dna_blend=0.00, alpha=0.23)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=2048 backend=triton\n",
            "2048       | DMoAH (INT8)             | 19.84           | 201.58       | 412839.36       | 203.56          | 6          | 0.60       | sparse     | triton (max_rows=268, density=0.13, unique=32, int8, tokens=0.60, dna_blend=0.00, alpha=0.23)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=2048 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "4096       | Standard Attention       | 92.30           | 43.34        | 177505.38       | 1114.08         | -          | -          | -          | torch.nn                            \n",
            "4096       | DMoAH (BF16)             | 40.30           | 99.25        | 406520.01       | 360.83          | 6          | 0.60       | sparse     | triton (max_rows=522, density=0.13, unique=32, tokens=0.60, dna_blend=0.00, alpha=0.48)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=4096 backend=triton\n",
            "4096       | DMoAH (INT8)             | 46.29           | 86.42        | 353972.19       | 413.28          | 6          | 0.60       | sparse     | triton (max_rows=520, density=0.13, unique=32, int8, tokens=0.60, dna_blend=0.00, alpha=0.48)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=4096 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "8192       | Standard Attention       | 88.02           | 11.36        | 93066.07        | 1098.54         | -          | -          | -          | torch.nn                            \n",
            "8192       | DMoAH (BF16)             | 25.80           | 38.76        | 317552.13       | 298.36          | 6          | 0.45       | sparse     | triton (max_rows=744, density=0.09, unique=32, tokens=0.45, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=8192 backend=triton\n",
            "8192       | DMoAH (INT8)             | 29.51           | 33.88        | 277558.10       | 324.14          | 6          | 0.46       | sparse     | triton (max_rows=753, density=0.09, unique=32, int8, tokens=0.46, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=8192 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "16384      | Standard Attention       | 326.75          | 3.06         | 50142.46        | 4209.97         | -          | -          | -          | torch.nn                            \n",
            "16384      | DMoAH (BF16)             | 71.00           | 14.08        | 230756.59       | 816.44          | 5          | 0.45       | sparse     | triton (max_rows=1259, density=0.08, unique=32, tokens=0.45, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=16384 backend=triton\n",
            "16384      | DMoAH (INT8)             | 74.50           | 13.42        | 219909.34       | 868.67          | 5          | 0.46       | sparse     | triton (max_rows=1237, density=0.08, unique=32, int8, tokens=0.46, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=16384 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "[Notice] Standard attention hit its limit at seq_len=32768 (last safe ~16384).\n",
            "32768      | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "32768      | DMoAH (BF16)             | 121.83          | 8.21         | 268955.38       | 936.42          | 2          | 0.21       | flux       | triton (max_rows=509, density=0.02, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=32768 backend=triton\n",
            "32768      | DMoAH (INT8)             | 121.68          | 8.22         | 269295.15       | 936.47          | 2          | 0.21       | flux       | triton (max_rows=500, density=0.02, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=32768 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "65536      | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "65536      | DMoAH (BF16)             | 241.91          | 4.13         | 270913.64       | 1493.82         | 2          | 0.21       | flux       | triton (max_rows=1181, density=0.02, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=65536 backend=triton\n",
            "65536      | DMoAH (INT8)             | 242.76          | 4.12         | 269964.96       | 1493.72         | 2          | 0.21       | flux       | triton (max_rows=1224, density=0.02, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=65536 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "131072     | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "131072     | DMoAH (BF16)             | 491.76          | 2.03         | 266537.72       | 2788.11         | 2          | 0.21       | flux       | triton (max_rows=4449, density=0.03, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=131072 backend=triton\n",
            "131072     | DMoAH (INT8)             | 494.81          | 2.02         | 264894.07       | 2789.37         | 2          | 0.21       | flux       | triton (max_rows=6196, density=0.05, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=131072 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "262144     | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "262144     | DMoAH (BF16)             | 1012.47         | 0.99         | 258915.84       | 5543.32         | 2          | 0.21       | flux       | triton (max_rows=11306, density=0.04, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=262144 backend=triton\n",
            "262144     | DMoAH (INT8)             | 1016.41         | 0.98         | 257912.43       | 5544.19         | 2          | 0.21       | flux       | triton (max_rows=17804, density=0.07, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=262144 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "524288     | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "524288     | DMoAH (BF16)             | 2068.10         | 0.48         | 253511.32       | 11054.31        | 2          | 0.21       | flux       | triton (max_rows=38733, density=0.07, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (BF16) seq_len=524288 backend=triton\n",
            "524288     | DMoAH (INT8)             | 2052.08         | 0.49         | 255490.92       | 11058.65        | 2          | 0.21       | flux       | triton (max_rows=18937, density=0.04, unique=32, tokens=0.21, dna_blend=0.00, alpha=1.00)\n",
            "[AutoLog] captured DMoAH (INT8) seq_len=524288 backend=triton\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "1048576    | Standard Attention       | OOM             | -            | -               | -               | -          | -          | -          | std-limit                           \n",
            "1048576    | DMoAH (BF16)             | OOM             | -            | -               | -               | -          | -          | auto       | dmoah-skip                          \n",
            "[Notice] DMoAH (BF16) skipping seq_len=1048576 due to limit.\n",
            "1048576    | DMoAH (BF16)             | OOM             | -            | -               | -               | -          | -          | -          | dmoah-limit                         \n",
            "1048576    | DMoAH (INT8)             | OOM             | -            | -               | -               | -          | -          | auto       | dmoah-skip                          \n",
            "[Notice] DMoAH (INT8) skipping seq_len=1048576 due to limit.\n",
            "1048576    | DMoAH (INT8)             | OOM             | -            | -               | -               | -          | -          | -          | dmoah-limit                         \n",
            "1048576    | DMoAH + FluxChunk        | 10067.67        | 0.10         | 104152.79       | 3977.82         | 2          | 0.05       | flux       | fluxchunk/triton (chunk=2171.1ms,...\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n",
            "[Summary] Standard safe length: 16384; DMoAH safe length: 524288; Chunked Flux safe length: 1048576.\n",
            "[Plot] Saved latency plot to reports/tinytoy/tinytoy_summary_20251104-143755.png\n",
            "\n",
            "[Summary] Detailed JSON report saved to reports/tinytoy/tinytoy_summary_20251104-143755.json\n",
            "[Summary] Captured 27 AutoLog entries.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5149d2c2",
        "outputId": "733c9a7e-e7f8-4595-b0a5-5effec424da3"
      },
      "source": [
        "# This will show you a chunked 1 Mil context, with some interesting metrics. It's a neat proof of concept. Takes a second to load.\n",
        "!python scripts/chunked_flux_smoke.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunked Flux: 100% 16/16 [00:02<00:00,  7.71chunk/s]\n",
            "\n",
            "=== Chunked Flux Smoke Test ===\n",
            "device              : cuda\n",
            "sequence length     : 1000000\n",
            "retained tokens     : 32768 (3.28%)\n",
            "chunks processed    : 16\n",
            "stream latency (ms) : 2076.38\n",
            "stream throughput   : 481,607.44 tok/s\n",
            "final latency (ms)  : 112.54\n",
            "final throughput    : 291,164.72 tok/s\n",
            "final peak memory   : 1257.5 MB\n",
            "overall throughput  : 439,936.04 tok/s\n",
            "==============================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52166761",
        "outputId": "d85639b8-591c-41bc-f94f-72f58fd8bdbc"
      },
      "source": [
        "# Alright, this is some good stuff. Mechanical Needle in a Haystack tests, to prove the actual mechanics are sound. Testing teleportation across chunks\n",
        "# Also if the buffer isnt actually a word salad with a very bad drizzle of confusion. Look it over, play with it, punch it in. It's pretty fun to watch it go brrr when you mess with it.\n",
        "!python scripts/chunked_flux_tests.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "config: seq_len=8,192 chunk_len=1,024 buffer_tokens=8 per_chunk_budget=2 (keep ratio 1.000)\n",
            "=== Needle recall (high router score) ===\n",
            "retained sentinel? True | keep_indices=[1047, 2891, 3937, 4357, 4369, 5231, 7174, 7233]\n",
            "\n",
            "=== DNA teleportation hypothesis ===\n",
            "without DNA retained? False | with DNA retained? True\n",
            "\n",
            "=== Ordering sanity (RoPE alignment) ===\n",
            "keep_indices monotonic? True\n",
            "\n",
            "=== Needle cluster stress test ===\n",
            "[cluster] needles kept 1/10 (recall=0.10) with per_chunk_budget=1\n",
            "[cluster-adapt] needles kept 8/10 (recall=0.80) margin=0.1 max_extra=8\n",
            "\n",
            "=== Fading signal sweep ===\n",
            "[fading] sensitivity sweep:\n",
            "  boost=5.00 -> recall 100.00% (20/20)\n",
            "  boost=2.00 -> recall 100.00% (20/20)\n",
            "  boost=1.00 -> recall 100.00% (20/20)\n",
            "  boost=0.50 -> recall 100.00% (20/20)\n",
            "  boost=0.25 -> recall 55.00% (11/20)\n",
            "  boost=0.10 -> recall 0.00% (0/20)\n",
            "\n",
            "=== Jigsaw puzzle synthesis ===\n",
            "[jigsaw] both clues retained? True | keep_indices=[512, 1287, 2313, 2641, 4823, 5179, 6712, 7851]\n",
            "\n",
            "=== Sample text demo ===\n",
            "[sample] Loaded 106 tokens from /content/Proteus-Attention/scripts/samples/flux_sample.txt (limit=none)\n",
            "[sample] Retained 2/2 sentinel anchors (chunk_len=106, buffer_tokens=8).\n",
            "  [✓] idx=54: unrelated trivia. Even so, the Titan vault code 9876 needed to\n",
            "  [✓] idx=88: condensed buffer, they confirmed the Titan vault code 9876 still surfaced.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61dd468a",
        "outputId": "ae3d5d32-4ee8-40b7-c4b7-adb34205c89b"
      },
      "source": [
        "# For the love of the machine god, do not try to go to 10 Million yet. My auto shunt system to push the SYS RAM Excess into a file on disk, is not in this to the right degree yet.\n",
        "# You can actually watch the RAM go up as you test this. It's pretty cool.\n",
        "!python scripts/run_fluxchunk_sweep.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FluxChunk] seq_len=1,000,000 buffer=1,000 alpha=1.000 device=cuda\n",
            "  chunk: 1038.4 ms | 962,974 tok/s\n",
            "  final: 2917.0 ms | 343 tok/s\n",
            "  peak VRAM: 113.4 MB\n",
            "  total: 3971.0 ms | 251,825 tok/s\n",
            "  retained=1,000 (0.0010) | fallback=False\n",
            "[FluxChunk] seq_len=2,000,000 buffer=10,000 alpha=1.000 device=cuda\n",
            "  chunk: 2497.4 ms | 800,828 tok/s\n",
            "  final: 17.3 ms | 578,983 tok/s\n",
            "  peak VRAM: 225.7 MB\n",
            "  total: 2525.4 ms | 791,942 tok/s\n",
            "  retained=10,000 (0.0050) | fallback=False\n",
            "[FluxChunk] seq_len=3,000,000 buffer=30,000 alpha=1.000 device=cuda\n",
            "  chunk: 2860.2 ms | 1,048,866 tok/s\n",
            "  final: 29.5 ms | 1,018,330 tok/s\n",
            "  peak VRAM: 372.9 MB\n",
            "  total: 2909.5 ms | 1,031,101 tok/s\n",
            "  retained=30,000 (0.0100) | fallback=False\n",
            "[FluxChunk] seq_len=4,000,000 buffer=80,000 alpha=1.000 device=cuda\n",
            "  chunk: 4061.8 ms | 984,789 tok/s\n",
            "  final: 71.3 ms | 1,121,594 tok/s\n",
            "  peak VRAM: 694.3 MB\n",
            "  total: 4190.2 ms | 954,599 tok/s\n",
            "  retained=80,000 (0.0200) | fallback=False\n",
            "[FluxChunk] seq_len=5,000,000 buffer=100,000 alpha=1.000 device=cuda\n",
            "  chunk: 5511.9 ms | 907,127 tok/s\n",
            "  final: 89.0 ms | 1,123,305 tok/s\n",
            "  peak VRAM: 889.8 MB\n",
            "  total: 5677.9 ms | 880,602 tok/s\n",
            "  retained=100,000 (0.0200) | fallback=False\n",
            "[FluxChunk] seq_len=6,000,000 buffer=120,000 alpha=1.000 device=cuda\n",
            "Host allocation 2929.7 MB is near available RAM 3049.6 MB.\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}